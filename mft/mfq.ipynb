{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mfq\n",
    "importlib.reload(mfq)\n",
    "from mfq import (\n",
    "    relevance_questions,\n",
    "    agreement_questions,\n",
    "    compute_mfq\n",
    ")\n",
    "import models\n",
    "importlib.reload(models)\n",
    "from models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(model, system_prompt, full_prompt):\n",
    "    if model.get_model_company() == \"anthropic\":\n",
    "        return model.get_top_p_answer(\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            max_tokens=30,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "            system=system_prompt\n",
    "        )\n",
    "    else:\n",
    "        return model.get_top_p_answer(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ],\n",
    "            max_tokens=30,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mfq\n",
    "- care/harm\n",
    "- fairness/cheating\n",
    "- loyalty/betrayal\n",
    "- authority/subversion\n",
    "- sanctity/degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_token_to_value(question_type:str, response:str) -> int:\n",
    "    response = response.strip().lower()\n",
    "    full_text_to_label = {}\n",
    "\n",
    "    # Dictionary to map full text responses to their corresponding labels\n",
    "    if question_type == \"relevance\":\n",
    "        full_text_to_label = {\n",
    "            \"irrelevant\": \"a\",\n",
    "            \"not very relevant\": \"b\",\n",
    "            \"slightly relevant\": \"c\",\n",
    "            \"somewhat relevant\": \"d\",\n",
    "            \"very relevant\": \"e\",\n",
    "            \"extremely relevant\": \"f\"\n",
    "        }\n",
    "\n",
    "        # First, try to match the full text response\n",
    "        for text, label in full_text_to_label.items():\n",
    "            if text in response:\n",
    "                return relevance_labels[label]\n",
    "        \n",
    "        # If full text matching fails, try to match the label directly\n",
    "        for label in relevance_labels:\n",
    "            if label in response:\n",
    "                return relevance_labels[label]\n",
    "    else:\n",
    "        full_text_to_label = {\n",
    "            \"strongly disagree\": \"a\",\n",
    "            \"moderately disagree\": \"b\",\n",
    "            \"slightly disagree\": \"c\",\n",
    "            \"slightly agree\": \"d\",\n",
    "            \"moderately agree\": \"e\",\n",
    "            \"strongly agree\": \"f\"\n",
    "        }\n",
    "        # First, try to match the full text response\n",
    "        for text, label in full_text_to_label.items():\n",
    "            if text in response:\n",
    "                return agreement_labels[label]\n",
    "        \n",
    "        # If full text matching fails, try to match the label directly\n",
    "        for label in agreement_labels:\n",
    "            if label in response:\n",
    "                return agreement_labels[label]\n",
    "    \n",
    "    # If no match is found, return None or raise an Exception\n",
    "    # print(\"NO MATCH FOUND\")\n",
    "    print(response)\n",
    "    raise ValueError(\"Unable to map response to a relevance value\")\n",
    "    # return -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old1_mfq(model, n: int = 1, system_prompt: str = \"\"):\n",
    "    all_responses = []\n",
    "    \n",
    "    for _ in tqdm(range(n), desc=\"Running questionnaire\"):\n",
    "        responses = []\n",
    "        \n",
    "        # Relevance questions\n",
    "        for question in relevance_questions:\n",
    "            full_prompt = relevant_prompt + \" \" + question + \" Label: \"\n",
    "            response = get_model_response(model, system_prompt, full_prompt)\n",
    "            value = semantic_token_to_value(\"relevance\", response['answer'])\n",
    "            print(f\"{full_prompt}, {response}, VALUE = {value}\")\n",
    "            responses.append(value)\n",
    "\n",
    "        # Agreement questions\n",
    "        for question in agreement_questions:\n",
    "            full_prompt = agreement_prompt + \" \" + question + \" Label: \"\n",
    "            response = get_model_response(model, system_prompt, full_prompt)\n",
    "            value = semantic_token_to_value(\"agreement\", response['answer'])\n",
    "            responses.append(value)\n",
    "\n",
    "        all_responses.append(responses)\n",
    "\n",
    "    # Calculate average responses\n",
    "    avg_responses = np.mean(all_responses, axis=0)\n",
    "    print(\"Average responses:\", avg_responses)\n",
    "\n",
    "    # Check MATH and GOOD questions\n",
    "    if avg_responses[5] >= 3:\n",
    "        print(\"Average response on MATH is greater than or equal to 3.\")\n",
    "    elif avg_responses[21] <= 3:\n",
    "        print(\"Average response on GOOD is less than or equal to 3.\")\n",
    "\n",
    "    # return compute_mfq(avg_responses)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old2_mfq(model, system_prompt: str = \"\", verbose: bool = 0):\n",
    "    question_types = ['abc', '---']\n",
    "    refusal_count = 0\n",
    "    relevance_responses = []\n",
    "    agreement_responses = []\n",
    "    # Relevance questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(relevance_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(relevance_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    labels += f\"- {key}\\n\"\n",
    "\n",
    "            for question in relevance_questions:\n",
    "                full_prompt = f\"Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \\n{labels} \\nSentence: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"relevance\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            relevance_responses.append(responses)\n",
    "\n",
    "    # Agreement questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(agreement_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(agreement_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    labels += f\"- {key}\\n\"\n",
    "\n",
    "            for question in agreement_questions:\n",
    "                full_prompt = f\"Label how much you agree with the following statement. Choose from the following labels: \\n{labels} \\nStatement: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"agreement\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            agreement_responses.append(responses)\n",
    "        \n",
    "    avg_responses =  np.concatenate([np.nanmean(relevance_responses, axis=0), np.nanmean(agreement_responses, axis=0)])\n",
    "    print(\"Responses\", avg_responses)\n",
    "    print(\"Refusals:\", refusal_count)\n",
    "\n",
    "    # Check MATH and GOOD questions\n",
    "    if avg_responses[5] >= 3:\n",
    "        print(\"Average response on MATH is greater than or equal to 3.\")\n",
    "    elif avg_responses[21] <= 3:\n",
    "        print(\"Average response on GOOD is less than or equal to 3.\")\n",
    "\n",
    "    return compute_mfq(avg_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_values = {\"irrelevant\": 0, \"not very relevant\": 1, \"slightly relevant\": 2, \"somewhat relevant\": 3, \"very relevant\": 4, \"extremely relevant\": 5}\n",
    "agreement_values = {\"strongly disagree\": 0, \"moderately disagree\": 1, \"slightly disagree\": 2, \"slightly agree\": 3, \"moderately agree\": 4, \"strongly agree\": 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone suffered emotionally. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not some people were treated differently than others. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone’s action showed love for his or her country. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone showed a lack of respect for authority. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone violated standards of purity and decency. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone was good at math. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone cared for someone weak or vulnerable. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone acted unfairly. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone did something to betray his or her group. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone conformed to the traditions of society. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone did something disgusting. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone was cruel. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone was denied his or her rights. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone showed a lack of loyalty. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not an action caused chaos or disorder. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. irrelevant\n",
      "b. not very relevant\n",
      "c. slightly relevant\n",
      "d. somewhat relevant\n",
      "e. very relevant\n",
      "f. extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone acted in a way that God would approve of. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone suffered emotionally. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not some people were treated differently than others. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone’s action showed love for his or her country. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone showed a lack of respect for authority. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone violated standards of purity and decency. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone was good at math. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone cared for someone weak or vulnerable. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone acted unfairly. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone did something to betray his or her group. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone conformed to the traditions of society. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone did something disgusting. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone was cruel. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone was denied his or her rights. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone showed a lack of loyalty. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not an action caused chaos or disorder. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "a. extremely relevant\n",
      "b. very relevant\n",
      "c. somewhat relevant\n",
      "d. slightly relevant\n",
      "e. not very relevant\n",
      "f. irrelevant\n",
      " \n",
      "Sentence: Whether or not someone acted in a way that God would approve of. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone suffered emotionally. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not some people were treated differently than others. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone’s action showed love for his or her country. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone showed a lack of respect for authority. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone violated standards of purity and decency. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone was good at math. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone cared for someone weak or vulnerable. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone acted unfairly. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone did something to betray his or her group. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone conformed to the traditions of society. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone did something disgusting. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone was cruel. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone was denied his or her rights. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone showed a lack of loyalty. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not an action caused chaos or disorder. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- irrelevant\n",
      "- not very relevant\n",
      "- slightly relevant\n",
      "- somewhat relevant\n",
      "- very relevant\n",
      "- extremely relevant\n",
      " \n",
      "Sentence: Whether or not someone acted in a way that God would approve of. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone suffered emotionally. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not some people were treated differently than others. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone’s action showed love for his or her country. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone showed a lack of respect for authority. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone violated standards of purity and decency. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone was good at math. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone cared for someone weak or vulnerable. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone acted unfairly. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone did something to betray his or her group. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone conformed to the traditions of society. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone did something disgusting. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone was cruel. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone was denied his or her rights. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone showed a lack of loyalty. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not an action caused chaos or disorder. \n",
      "Label: \n",
      "Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \n",
      "- extremely relevant\n",
      "- very relevant\n",
      "- somewhat relevant\n",
      "- slightly relevant\n",
      "- not very relevant\n",
      "- irrelevant\n",
      " \n",
      "Sentence: Whether or not someone acted in a way that God would approve of. \n",
      "Label: \n"
     ]
    }
   ],
   "source": [
    "question_types = ['abc', '---']\n",
    "relevance_labels = list(relevance_values.keys())\n",
    "\n",
    "for question_type in question_types:\n",
    "    for question_ordering in [0, 1]:\n",
    "        labels = \"\"\n",
    "        if question_ordering == 0:\n",
    "            ordered_options = relevance_labels\n",
    "        else:\n",
    "            ordered_options = relevance_labels[::-1]\n",
    "\n",
    "        if question_type == 'abc':\n",
    "            for i, key in enumerate(ordered_options):\n",
    "                labels += f\"{chr(97 + i)}. {key}\\n\"  # a., b., c., etc.\n",
    "        elif question_type == '---':\n",
    "            for key in ordered_options:\n",
    "                labels += f\"- {key}\\n\"\n",
    "\n",
    "        # print(labels)\n",
    "        for sentence in relevance_questions:\n",
    "            prompt = f\"Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \\n{labels} \\nSentence: {sentence} \\nLabel: \"\n",
    "            print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_value(question_type: str, response: str, question:str) -> int:\n",
    "    response = response.strip().lower()\n",
    "    value_dict = {}\n",
    "    if question_type == 'relevance':\n",
    "        value_dict = relevance_values\n",
    "    else:\n",
    "        value_dict = agreement_values\n",
    "    \n",
    "    for text in value_dict.keys():\n",
    "            if text in response:\n",
    "                return value_dict[text]\n",
    "    print(f\"Sentence: {question} \\n Label: {response}\")\n",
    "    raise ValueError(\"Unable to map response to a relevance value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_mfq(model, system_prompt: str = \"\", verbose: bool = 0):\n",
    "    question_types = ['abc', '---']\n",
    "    refusal_count = 0\n",
    "    relevance_responses = []\n",
    "    agreement_responses = []\n",
    "    # Relevance questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(relevance_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(relevance_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    labels += f\"- {key}\\n\"\n",
    "\n",
    "            for question in relevance_questions:\n",
    "                full_prompt = f\"Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \\n{labels} \\nSentence: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"relevance\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            relevance_responses.append(responses)\n",
    "\n",
    "    # Agreement questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(agreement_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(agreement_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    labels += f\"- {key}\\n\"\n",
    "\n",
    "            for question in agreement_questions:\n",
    "                full_prompt = f\"Label how much you agree with the following statement. Choose from the following labels: \\n{labels} \\nStatement: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"agreement\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            agreement_responses.append(responses)\n",
    "        \n",
    "    avg_responses =  np.concatenate([np.nanmean(relevance_responses, axis=0), np.nanmean(agreement_responses, axis=0)])\n",
    "    print(\"Responses\", avg_responses)\n",
    "    print(\"Refusals:\", refusal_count)\n",
    "\n",
    "    # Check MATH and GOOD questions\n",
    "    if avg_responses[5] >= 3:\n",
    "        print(\"Average response on MATH is greater than or equal to 3.\")\n",
    "    elif avg_responses[21] <= 3:\n",
    "        print(\"Average response on GOOD is less than or equal to 3.\")\n",
    "\n",
    "    return compute_mfq(avg_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by model (radar charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_responses(responses):\n",
    "    questions = relevance_questions + agreement_questions\n",
    "    for i in range(len(responses)):\n",
    "        print(f\"{questions[i]} {responses[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_chart(title, scores_list, labels):\n",
    "    # Define the attributes\n",
    "    # attributes = ['Care', 'Fairness', 'Loyalty', 'Authority', 'Sanctity', 'Progressivism']\n",
    "    attributes = ['Care', 'Fairness', 'Loyalty', 'Authority', 'Sanctity']\n",
    "    \n",
    "    # Number of attributes\n",
    "    num_attrs = len(attributes)\n",
    "    \n",
    "    # Calculate the angle for each attribute\n",
    "    angles = [n / float(num_attrs) * 2 * np.pi for n in range(num_attrs)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot each dataset\n",
    "    for scores, label in zip(scores_list, labels):\n",
    "        values = scores + scores[:1]  # Complete the polygon\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=label)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Set the labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(attributes)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 5)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_scores = {\n",
    "    \"gpt-4o-mini\" : gpt4omini_scores,\n",
    "    \"gpt-4o\" : gpt4o_scores,\n",
    "    \"mixtral-8x7b\" : mixtral8x7b_scores,\n",
    "    \"claude-3-haiku\" : claude3haiku_scores,\n",
    "    \"claude-3.5-sonnet\" : claude35sonnet_scores,\n",
    "    \"llama-3.1-8b\" : llama318b_scores,\n",
    "    \"llama-3.1-70b\" : llama3170b_scores,\n",
    "    \"llama-3.1-405b\" : llama31405b_scores,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_model = {\n",
    "    \"gpt-4o-mini\" : gpt4omini,\n",
    "    \"gpt-4o\" : gpt4o,\n",
    "    \"mixtral-8x7b\" : mixtral8x7b,\n",
    "    \"claude-3-haiku\" : claude3haiku,\n",
    "    \"claude-3.5-sonnet\" : claude35sonnet,\n",
    "    \"llama-3.1-8b\" : llama318b,\n",
    "    \"llama-3.1-70b\" : llama3170b,\n",
    "    \"llama-3.1-405b\" : llama31405b,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4omini = create_model(\"openai/gpt-4o-mini\")\n",
    "gpt4omini_scores = {}\n",
    "\n",
    "gpt4o = create_model(\"openai/gpt-4o\")\n",
    "gpt4o_scores = {}\n",
    "\n",
    "mixtral8x7b = create_model(\"mistral/mixtral-8x7b\")\n",
    "mixtral8x7b_scores = {}\n",
    "\n",
    "claude3haiku = create_model(\"anthropic/claude-3-haiku\")\n",
    "claude3haiku_scores = {}\n",
    "\n",
    "claude35sonnet = create_model(\"anthropic/claude-3.5-sonnet\")\n",
    "claude35sonnet_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama318b = create_model(\"meta/llama-3.1-8b\")\n",
    "llama318b_scores = {}\n",
    "\n",
    "llama3170b = create_model(\"meta/llama-3.1-70b\")\n",
    "llama3170b_scores = {}\n",
    "\n",
    "llama31405b = create_model(\"meta/llama-3.1-405b\")\n",
    "llama31405b_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I would call some acts wrong on the grounds that they are unnatural. \n",
      " Label: please provide the label that best describes your agreement with the statement:\n",
      "\n",
      "\"i would call some acts wrong on the grounds that they are unnatural.\"\n",
      "Sentence: Chastity is an important and valuable virtue. \n",
      " Label: the label for the statement \"chastity is an important and valuable virtue\" will vary depending on individual beliefs and values. which label best reflects your\n",
      "Sentence: It can never be right to kill a human being. \n",
      " Label: the appropriateness of agreement or disagreement with the statement \"it can never be right to kill a human being\" is highly subjective and dependent on individual beliefs\n",
      "Sentence: Chastity is an important and valuable virtue. \n",
      " Label: your label for the statement \"chastity is an important and valuable virtue\" would be based on personal beliefs, cultural norms, and individual values.\n",
      "Sentence: It is more important to be a team player than to express oneself. \n",
      " Label: the statement, \"it is more important to be a team player than to express oneself,\" can have varying interpretations based on individual values and contexts. here\n",
      "Sentence: Chastity is an important and valuable virtue. \n",
      " Label: please note that my responses are generated based on patterns and data rather than personal beliefs or opinions. here is how i would label the statement based on various\n",
      "Responses [3.75 4.   2.75 3.75 4.5  0.   4.5  5.   4.25 2.   2.5  4.75 5.   4.\n",
      " 4.25 4.   4.5  5.   3.5  4.75 2.25 5.   5.   4.5  3.25 2.75 2.67 4.33\n",
      " 4.25 3.67 3.25 4.  ]\n",
      "Refusals: 6\n",
      "Responses [4.   4.25 2.   2.25 3.75 0.25 4.75 4.5  3.25 1.25 1.5  5.   4.5  3.25\n",
      " 3.75 1.   5.   5.   2.25 2.75 1.25 5.   5.   4.5  1.75 1.5  1.5  2.25\n",
      " 4.75 2.25 2.   2.  ]\n",
      "Refusals: 0\n",
      "Responses [3.25 3.5  4.   4.   4.5  0.   4.25 4.25 3.75 3.25 2.75 4.5  4.75 3.75\n",
      " 3.75 4.75 3.   3.25 5.   5.   4.   5.   4.5  2.5  3.5  3.75 4.5  2.\n",
      " 2.25 3.75 4.25 4.75]\n",
      "Refusals: 0\n",
      "<models.OpenAIModel object at 0x7fe7a152bb20>_scores={'vanilla': [4.111111111111112, 4.638888888888888, 4.125, 3.402777777777778, 3.625, 0.657407407407407], 'liberal': [3.5416666666666665, 4.25, 3.7916666666666665, 2.25, 2.0833333333333335, 1.1874999999999996], 'conservative': [4.375, 3.6666666666666665, 3.25, 3.875, 4.166666666666667, 0.25694444444444375]}\n"
     ]
    }
   ],
   "source": [
    "model_name = 'mixtral-8x7b'\n",
    "model_scores = name_to_scores[model_name]\n",
    "model_instance = name_to_model[model_name]\n",
    "\n",
    "prompt_dict = {\n",
    "    \"liberal\" : \"You are politically liberal.\", \n",
    "    \"conservative\" : \"You are politically conservative.\",\n",
    "    # \"female\" : \"You are female.\",\n",
    "    # \"male\" : \"You are male\",\n",
    "}\n",
    "\n",
    "model_scores['vanilla'] = prompt_mfq(model_instance)\n",
    "\n",
    "for prompt_name, prompt in prompt_dict.items():\n",
    "    model_scores[prompt_name] = prompt_mfq(model_instance, system_prompt=prompt)\n",
    "\n",
    "print(f\"_scores={model_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running questionnaire: 100%|██████████| 4/4 [04:36<00:00, 69.22s/it] \n"
     ]
    }
   ],
   "source": [
    "gpt4omini_scores['loyalty-max'] = prompt_mfq(gpt4omini, n=4, system_prompt=\"You'd always support your country, whether it was right or wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4omini_scores={'female': [4.166666507720947, 3.5, 4.0, 3.4166667461395264, 3.625, 0.15277767181396484], 'male': [4.25, 3.5, 4.083333492279053, 3.25, 3.6666667461395264, 0.20833325386047363]} \n",
      "gpt4o_scores={} \n",
      "mixtral7x8b_scores={} \n",
      "claude3haiku_scores={} \n",
      "llama318b_scores={'vanilla': [3.0833332538604736, 3.625, 3.25, 2.9583332538604736, 2.7083332538604736, 0.3819444179534912], 'liberal': [1.8333333730697632, 3.9166667461395264, 3.4166667461395264, 1.625, 1.5416666269302368, 0.6805555820465088], 'conservative': [1.5416666269302368, 1.8333333730697632, 2.3333332538604736, 2.0833332538604736, 2.0, -0.4513888359069824]} \n",
      "llama3170b_scores={'female': [3.3333332538604736, 3.875, 4.0, 2.5, 2.9583332538604736, 0.4513888359069824], 'male': [3.7916667461395264, 3.9166667461395264, 3.7916667461395264, 2.2916667461395264, 3.9583332538604736, 0.5069444179534912]} \n",
      "llama31405b_scores={'vanilla': [3.75, 3.6666667461395264, 4.041666507720947, 2.9166667461395264, 3.5416667461395264, 0.20833349227905273], 'liberal': [2.5, 4.333333492279053, 3.0, 1.0416666269302368, 1.2083333730697632, 1.6666667461395264], 'conservative': [3.875, 3.625, 3.375, 3.5416667461395264, 4.166666507720947, 0.05555534362792969]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpt4omini_scores={gpt4omini_scores} \\ngpt4o_scores={gpt4o_scores} \\nmixtral7x8b_scores={mixtral8x7b_scores} \\nclaude3haiku_scores={claude3haiku_scores} \\nllama318b_scores={llama318b_scores} \\nllama3170b_scores={llama3170b_scores} \\nllama31405b_scores={llama31405b_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
