{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import questionnaire\n",
    "importlib.reload(questionnaire)\n",
    "from questionnaire import (\n",
    "    relevance_questions,\n",
    "    relevance_labels,\n",
    "    relevant_prompt,\n",
    "    agreement_questions,\n",
    "    agreement_labels,\n",
    "    agreement_prompt,\n",
    "    compute_mfq\n",
    ")\n",
    "import models\n",
    "importlib.reload(models)\n",
    "from models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(model, system_prompt, full_prompt):\n",
    "    if model.get_model_company() == \"anthropic\":\n",
    "        return model.get_top_p_answer(\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            max_tokens=30,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "            system=system_prompt\n",
    "        )\n",
    "    else:\n",
    "        return model.get_top_p_answer(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ],\n",
    "            max_tokens=30,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mfq\n",
    "- care/harm\n",
    "- fairness/cheating\n",
    "- loyalty/betrayal\n",
    "- authority/subversion\n",
    "- sanctity/degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_mfq(model, n: int = 1, system_prompt: str = \"\"):\n",
    "    all_responses = []\n",
    "    \n",
    "    for _ in tqdm(range(n), desc=\"Running questionnaire\"):\n",
    "        responses = []\n",
    "        \n",
    "        # Relevance questions\n",
    "        for question in relevance_questions:\n",
    "            full_prompt = relevant_prompt + \" \" + question + \" Label: \"\n",
    "            response = get_model_response(model, system_prompt, full_prompt)\n",
    "            value = semantic_token_to_value(\"relevance\", response['answer'])\n",
    "            responses.append(value)\n",
    "\n",
    "        # Agreement questions\n",
    "        for question in agreement_questions:\n",
    "            full_prompt = agreement_prompt + \" \" + question + \" Label: \"\n",
    "            response = get_model_response(model, system_prompt, full_prompt)\n",
    "            value = semantic_token_to_value(\"agreement\", response['answer'])\n",
    "            responses.append(value)\n",
    "\n",
    "        all_responses.append(responses)\n",
    "\n",
    "    # Calculate average responses\n",
    "    avg_responses = np.mean(all_responses, axis=0)\n",
    "    # print(\"Average responses:\", avg_responses)\n",
    "\n",
    "    # Check MATH and GOOD questions\n",
    "    if avg_responses[5] >= 3:\n",
    "        print(\"Average response on MATH is greater than or equal to 3.\")\n",
    "    elif avg_responses[21] <= 3:\n",
    "        print(\"Average response on GOOD is less than or equal to 3.\")\n",
    "\n",
    "    avg_responses_tensor = torch.tensor(avg_responses).float()\n",
    "    return compute_mfq(avg_responses_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_token_to_value(question_type:str, response:str) -> int:\n",
    "    response = response.strip().lower()\n",
    "    full_text_to_label = {}\n",
    "\n",
    "    # Dictionary to map full text responses to their corresponding labels\n",
    "    if question_type == \"relevance\":\n",
    "        full_text_to_label = {\n",
    "            \"irrelevant\": \"a\",\n",
    "            \"not very relevant\": \"b\",\n",
    "            \"slightly relevant\": \"c\",\n",
    "            \"somewhat relevant\": \"d\",\n",
    "            \"very relevant\": \"e\",\n",
    "            \"extremely relevant\": \"f\"\n",
    "        }\n",
    "\n",
    "        # First, try to match the full text response\n",
    "        for text, label in full_text_to_label.items():\n",
    "            if text in response:\n",
    "                return relevance_labels[label]\n",
    "        \n",
    "        # If full text matching fails, try to match the label directly\n",
    "        for label in relevance_labels:\n",
    "            if label in response:\n",
    "                return relevance_labels[label]\n",
    "    else:\n",
    "        full_text_to_label = {\n",
    "            \"strongly disagree\": \"a\",\n",
    "            \"moderately disagree\": \"b\",\n",
    "            \"slightly disagree\": \"c\",\n",
    "            \"slightly agree\": \"d\",\n",
    "            \"moderately agree\": \"e\",\n",
    "            \"strongly agree\": \"f\"\n",
    "        }\n",
    "        # First, try to match the full text response\n",
    "        for text, label in full_text_to_label.items():\n",
    "            if text in response:\n",
    "                return agreement_labels[label]\n",
    "        \n",
    "        # If full text matching fails, try to match the label directly\n",
    "        for label in agreement_labels:\n",
    "            if label in response:\n",
    "                return agreement_labels[label]\n",
    "    \n",
    "    # If no match is found, return None or raise an Exception\n",
    "    # print(\"NO MATCH FOUND\")\n",
    "    raise ValueError(\"Unable to map response to a relevance value\")\n",
    "    # return -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_responses(responses):\n",
    "    questions = relevance_questions + agreement_questions\n",
    "    for i in range(len(responses)):\n",
    "        print(f\"{questions[i]} {responses[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_chart(title, scores_list, labels):\n",
    "    # Define the attributes\n",
    "    # attributes = ['Care', 'Fairness', 'Loyalty', 'Authority', 'Sanctity', 'Progressivism']\n",
    "    attributes = ['Care', 'Fairness', 'Loyalty', 'Authority', 'Sanctity']\n",
    "    \n",
    "    # Number of attributes\n",
    "    num_attrs = len(attributes)\n",
    "    \n",
    "    # Calculate the angle for each attribute\n",
    "    angles = [n / float(num_attrs) * 2 * np.pi for n in range(num_attrs)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot each dataset\n",
    "    for scores, label in zip(scores_list, labels):\n",
    "        values = scores + scores[:1]  # Complete the polygon\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=label)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Set the labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(attributes)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 5)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_scores = {\n",
    "    \"gpt-4o-mini\" : gpt4omini_scores,\n",
    "    \"gpt-4o\" : gpt4o_scores,\n",
    "    \"mixtral-8x7b\" : mixtral8x7b_scores,\n",
    "    \"claude-3-haiku\" : claude3haiku_scores,\n",
    "    \"claude-3.5-sonnet\" : claude35sonnet_scores,\n",
    "    \"llama-3.1-8b\" : llama318b_scores,\n",
    "    \"llama-3.1-70b\" : llama3170b_scores,\n",
    "    \"llama-3.1-405b\" : llama31405b_scores,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_model = {\n",
    "    \"gpt-4o-mini\" : gpt4omini,\n",
    "    \"gpt-4o\" : gpt4o,\n",
    "    \"mixtral-8x7b\" : mixtral8x7b,\n",
    "    \"claude-3-haiku\" : claude3haiku,\n",
    "    \"claude-3.5-sonnet\" : claude35sonnet,\n",
    "    \"llama-3.1-8b\" : llama318b,\n",
    "    \"llama-3.1-70b\" : llama3170b,\n",
    "    \"llama-3.1-405b\" : llama31405b,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4omini = create_model(\"openai/gpt-4o-mini\")\n",
    "gpt4omini_scores = {}\n",
    "\n",
    "gpt4o = create_model(\"openai/gpt-4o\")\n",
    "gpt4o_scores = {}\n",
    "\n",
    "mixtral8x7b = create_model(\"mistral/mixtral-8x7b\")\n",
    "mixtral8x7b_scores = {}\n",
    "\n",
    "claude3haiku = create_model(\"anthropic/claude-3-haiku\")\n",
    "claude3haiku_scores = {}\n",
    "\n",
    "claude35sonnet = create_model(\"anthropic/claude-3.5-sonnet\")\n",
    "claude35sonnet_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama318b = create_model(\"meta/llama-3.1-8b\")\n",
    "llama318b_scores = {}\n",
    "\n",
    "llama3170b = create_model(\"meta/llama-3.1-70b\")\n",
    "llama3170b_scores = {}\n",
    "\n",
    "llama31405b = create_model(\"meta/llama-3.1-405b\")\n",
    "llama31405b_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = {\n",
    "    \"liberal\" : \"You are politically liberal.\", \n",
    "    \"conservative\" : \"You are politically conservative.\",\n",
    "    \"female\" : \"You are female.\",\n",
    "    \"male\" : \"You are male\",\n",
    "}\n",
    "\n",
    "model_name = 'gpt-4o'\n",
    "model_scores = name_to_scores[model_name]\n",
    "model_instance = name_to_model[model_name]\n",
    "\n",
    "for prompt_name, prompt in prompt_dict:\n",
    "    model_scores[prompt_name] = prompt_mfq(model_instance, n=4, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running questionnaire: 100%|██████████| 4/4 [04:36<00:00, 69.22s/it] \n"
     ]
    }
   ],
   "source": [
    "gpt4omini_scores['loyalty-max'] = prompt_mfq(gpt4omini, n=4, system_prompt=\"You'd always support your country, whether it was right or wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'female': [4.166666507720947, 3.5, 4.0, 3.4166667461395264, 3.625, 0.15277767181396484], 'male': [4.25, 3.5, 4.083333492279053, 3.25, 3.6666667461395264, 0.20833325386047363], 'loyalty-max': [3.4583332538604736, 3.375, 3.2083332538604736, 2.3333332538604736, 2.4583332538604736, 0.7499997615814209]}\n"
     ]
    }
   ],
   "source": [
    "print(gpt4omini_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4omini_scores={'female': [4.166666507720947, 3.5, 4.0, 3.4166667461395264, 3.625, 0.15277767181396484], 'male': [4.25, 3.5, 4.083333492279053, 3.25, 3.6666667461395264, 0.20833325386047363]} \n",
      "gpt4o_scores={} \n",
      "mixtral7x8b_scores={} \n",
      "claude3haiku_scores={} \n",
      "llama318b_scores={'vanilla': [3.0833332538604736, 3.625, 3.25, 2.9583332538604736, 2.7083332538604736, 0.3819444179534912], 'liberal': [1.8333333730697632, 3.9166667461395264, 3.4166667461395264, 1.625, 1.5416666269302368, 0.6805555820465088], 'conservative': [1.5416666269302368, 1.8333333730697632, 2.3333332538604736, 2.0833332538604736, 2.0, -0.4513888359069824]} \n",
      "llama3170b_scores={'female': [3.3333332538604736, 3.875, 4.0, 2.5, 2.9583332538604736, 0.4513888359069824], 'male': [3.7916667461395264, 3.9166667461395264, 3.7916667461395264, 2.2916667461395264, 3.9583332538604736, 0.5069444179534912]} \n",
      "llama31405b_scores={'vanilla': [3.75, 3.6666667461395264, 4.041666507720947, 2.9166667461395264, 3.5416667461395264, 0.20833349227905273], 'liberal': [2.5, 4.333333492279053, 3.0, 1.0416666269302368, 1.2083333730697632, 1.6666667461395264], 'conservative': [3.875, 3.625, 3.375, 3.5416667461395264, 4.166666507720947, 0.05555534362792969]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpt4omini_scores={gpt4omini_scores} \\ngpt4o_scores={gpt4o_scores} \\nmixtral7x8b_scores={mixtral8x7b_scores} \\nclaude3haiku_scores={claude3haiku_scores} \\nllama318b_scores={llama318b_scores} \\nllama3170b_scores={llama3170b_scores} \\nllama31405b_scores={llama31405b_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
