{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mfq\n",
    "importlib.reload(mfq)\n",
    "from mfq import (\n",
    "    relevance_questions,\n",
    "    agreement_questions,\n",
    "    compute_mfq\n",
    ")\n",
    "import models\n",
    "importlib.reload(models)\n",
    "from models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(model, system_prompt, full_prompt):\n",
    "    if model.get_model_company() == \"anthropic\":\n",
    "        return model.get_top_p_answer(\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            max_tokens=30,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "            system=system_prompt\n",
    "        )\n",
    "    else:\n",
    "        return model.get_top_p_answer(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ],\n",
    "            max_tokens=30,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mfq\n",
    "- care/harm\n",
    "- fairness/cheating\n",
    "- loyalty/betrayal\n",
    "- authority/subversion\n",
    "- sanctity/degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5}\n",
    "\n",
    "def semantic_token_to_value(question_type:str, response:str) -> int:\n",
    "    response = response.strip().lower()\n",
    "    full_text_to_label = {}\n",
    "\n",
    "    # Dictionary to map full text responses to their corresponding labels\n",
    "    if question_type == \"relevance\":\n",
    "        full_text_to_label = {\n",
    "            \"irrelevant\": \"a\",\n",
    "            \"not very relevant\": \"b\",\n",
    "            \"slightly relevant\": \"c\",\n",
    "            \"somewhat relevant\": \"d\",\n",
    "            \"very relevant\": \"e\",\n",
    "            \"extremely relevant\": \"f\"\n",
    "        }\n",
    "\n",
    "        # First, try to match the full text response\n",
    "        for text, label in full_text_to_label.items():\n",
    "            if text in response:\n",
    "                return labels[label]\n",
    "        \n",
    "        # If full text matching fails, try to match the label directly\n",
    "        for label in labels:\n",
    "            if label in response:\n",
    "                return labels[label]\n",
    "    else:\n",
    "        full_text_to_label = {\n",
    "            \"strongly disagree\": \"a\",\n",
    "            \"moderately disagree\": \"b\",\n",
    "            \"slightly disagree\": \"c\",\n",
    "            \"slightly agree\": \"d\",\n",
    "            \"moderately agree\": \"e\",\n",
    "            \"strongly agree\": \"f\"\n",
    "        }\n",
    "        # First, try to match the full text response\n",
    "        for text, label in full_text_to_label.items():\n",
    "            if text in response:\n",
    "                return labels[label]\n",
    "        \n",
    "        # If full text matching fails, try to match the label directly\n",
    "        for label in labels:\n",
    "            if label in response:\n",
    "                return labels[label]\n",
    "    \n",
    "    # If no match is found, return None or raise an Exception\n",
    "    # print(\"NO MATCH FOUND\")\n",
    "    print(response)\n",
    "    raise ValueError(\"Unable to map response to a relevance value\")\n",
    "    # return -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_prompt = \"Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: a. irrelevant, b. not very relevant, c. slightly relevant, d. somewhat relevant, e. very relevant, f. extremely relevant. \" \n",
    "agreement_prompt = \"Label the sentence with whether you agree or disagree. Choose from the following labels: a. strongly disagree, b. moderately disagree, c. slightly disagree, d. slightly agree, e. moderately agree, f. strongly agree.\"\n",
    "\n",
    "def old1_mfq(model, n: int = 1, system_prompt: str = \"\"):\n",
    "    all_responses = []\n",
    "    \n",
    "    for _ in tqdm(range(n), desc=\"Running questionnaire\"):\n",
    "        responses = []\n",
    "        \n",
    "        # Relevance questions\n",
    "        print(\"relevance\")\n",
    "        for question in relevance_questions:\n",
    "            full_prompt = relevant_prompt + \" \" + question + \" Label: \"\n",
    "            response = get_model_response(model, system_prompt, full_prompt)\n",
    "            value = semantic_token_to_value(\"relevance\", response['answer'])\n",
    "            print(f\"{full_prompt}, {response['answer']}, VALUE = {value}\")\n",
    "            responses.append(value)\n",
    "\n",
    "        # Agreement questions\n",
    "        print(\"agreement\")\n",
    "        for question in agreement_questions:\n",
    "            full_prompt = agreement_prompt + \" \" + question + \" Label: \"\n",
    "            response = get_model_response(model, system_prompt, full_prompt)\n",
    "            value = semantic_token_to_value(\"agreement\", response['answer'])\n",
    "            print(f\"{full_prompt}, {response['answer']}, VALUE = {value}\")\n",
    "            responses.append(value)\n",
    "            \n",
    "        all_responses.append(responses)\n",
    "\n",
    "    # Calculate average responses\n",
    "    avg_responses = np.mean(all_responses, axis=0)\n",
    "    print(\"Average responses:\", avg_responses)\n",
    "\n",
    "    # Check MATH and GOOD questions\n",
    "    if avg_responses[5] >= 3:\n",
    "        print(\"Average response on MATH is greater than or equal to 3.\")\n",
    "    elif avg_responses[21] <= 3:\n",
    "        print(\"Average response on GOOD is less than or equal to 3.\")\n",
    "\n",
    "    return compute_mfq(avg_responses, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old2_mfq(model, system_prompt: str = \"\", verbose: bool = 0):\n",
    "    question_types = ['abc', '---']\n",
    "    refusal_count = 0\n",
    "    relevance_responses = []\n",
    "    agreement_responses = []\n",
    "    # Relevance questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(relevance_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(relevance_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    labels += f\"- {key}\\n\"\n",
    "\n",
    "            for question in relevance_questions:\n",
    "                full_prompt = f\"Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \\n{labels} \\nSentence: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"relevance\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            relevance_responses.append(responses)\n",
    "\n",
    "    # Agreement questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(agreement_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(agreement_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    labels += f\"- {key}\\n\"\n",
    "\n",
    "            for question in agreement_questions:\n",
    "                full_prompt = f\"Label how much you agree with the following statement. Choose from the following labels: \\n{labels} \\nStatement: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"agreement\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            agreement_responses.append(responses)\n",
    "        \n",
    "    avg_responses =  np.concatenate([np.nanmean(relevance_responses, axis=0), np.nanmean(agreement_responses, axis=0)])\n",
    "    print(\"Responses\", avg_responses)\n",
    "    print(\"Refusals:\", refusal_count)\n",
    "\n",
    "    # Check MATH and GOOD questions\n",
    "    if avg_responses[5] >= 3:\n",
    "        print(\"Average response on MATH is greater than or equal to 3.\")\n",
    "    elif avg_responses[21] <= 3:\n",
    "        print(\"Average response on GOOD is less than or equal to 3.\")\n",
    "\n",
    "    return compute_mfq(avg_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_values = {\"irrelevant\": 0, \"not very relevant\": 1, \"slightly relevant\": 2, \"somewhat relevant\": 3, \"very relevant\": 4, \"extremely relevant\": 5}\n",
    "agreement_values = {\"strongly disagree\": 0, \"moderately disagree\": 1, \"slightly disagree\": 2, \"slightly agree\": 3, \"moderately agree\": 4, \"strongly agree\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. irrelevant, b. not very relevant, c. slightly relevant, d. somewhat relevant, e. very relevant, f. extremely relevant, \n",
      "a. extremely relevant, b. very relevant, c. somewhat relevant, d. slightly relevant, e. not very relevant, f. irrelevant, \n",
      "irrelevant, not very relevant, slightly relevant, somewhat relevant, very relevant, extremely relevant, \n",
      "extremely relevant, very relevant, somewhat relevant, slightly relevant, not very relevant, irrelevant, \n"
     ]
    }
   ],
   "source": [
    "question_types = ['abc', '---']\n",
    "relevance_labels = list(relevance_values.keys())\n",
    "\n",
    "for question_type in question_types:\n",
    "    for question_ordering in [0, 1]:\n",
    "        labels = \"\"\n",
    "        if question_ordering == 0:\n",
    "            ordered_options = relevance_labels\n",
    "        else:\n",
    "            ordered_options = relevance_labels[::-1]\n",
    "\n",
    "        if question_type == 'abc':\n",
    "            for i, key in enumerate(ordered_options):\n",
    "                labels += f\"{chr(97 + i)}. {key}, \"  # a., b., c., etc.\n",
    "        elif question_type == '---':\n",
    "            for key in ordered_options:\n",
    "                labels += f\"{key}, \"\n",
    "\n",
    "        print(labels)\n",
    "        # for sentence in relevance_questions:\n",
    "        #     prompt = f\"Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \\n{labels} \\nSentence: {sentence} \\nLabel: \"\n",
    "        #     print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_value(question_type: str, response: str, question:str) -> int:\n",
    "    response = response.strip().lower()\n",
    "    value_dict = {}\n",
    "    if question_type == 'relevance':\n",
    "        value_dict = relevance_values\n",
    "    else:\n",
    "        value_dict = agreement_values\n",
    "    \n",
    "    for text in value_dict.keys():\n",
    "            if text in response:\n",
    "                return value_dict[text]\n",
    "    print(f\"  Sentence: {question}\\n    Label: {response}\")\n",
    "    raise ValueError(\"Unable to map response to a relevance value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_mfq(model, system_prompt: str = \"\", verbose: bool = 0):\n",
    "    print(\"---\")\n",
    "    print(model.get_model_id().upper())\n",
    "\n",
    "    question_types = ['abc', '---']\n",
    "    refusal_count = 0\n",
    "    relevance_responses = []\n",
    "    agreement_responses = []\n",
    "    # Relevance questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            print(\"relevance\", question_type, question_ordering)\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(relevance_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(relevance_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    # labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "                    labels += f\"{chr(97 + i)}. {key}, \"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    # labels += f\"- {key}\\n\"\n",
    "                    labels += f\"{key}, \"\n",
    "\n",
    "            for question in relevance_questions:\n",
    "                full_prompt = f\"Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \\n{labels} \\nSentence: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"relevance\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            relevance_responses.append(responses)\n",
    "\n",
    "    # Agreement questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            print(\"agreement\", question_type, question_ordering)\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(agreement_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(agreement_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    # labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "                    labels += f\"{chr(97 + i)}. {key}, \"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    # labels += f\"- {key}\\n\"\n",
    "                    labels += f\"{key}, \"\n",
    "\n",
    "            for question in agreement_questions:\n",
    "                full_prompt = f\"Label how much you agree with the following statement. Choose from the following labels: \\n{labels} \\nStatement: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"agreement\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            agreement_responses.append(responses)\n",
    "        \n",
    "    avg_responses =  np.concatenate([np.nanmean(relevance_responses, axis=0), np.nanmean(agreement_responses, axis=0)])\n",
    "    print(\"Responses\", \" \".join(f\"{x:.2f}\" for x in avg_responses))\n",
    "    print(\"Refusals:\", refusal_count)\n",
    "\n",
    "    # Check MATH and GOOD questions\n",
    "    if avg_responses[5] >= 3:\n",
    "        print(\"Average response on MATH is greater than or equal to 3.\")\n",
    "    elif avg_responses[21] <= 3:\n",
    "        print(\"Average response on GOOD is less than or equal to 3.\")\n",
    "\n",
    "    return compute_mfq(avg_responses, refusal_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by model (radar charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_responses(responses):\n",
    "    questions = relevance_questions + agreement_questions\n",
    "    for i in range(len(responses)):\n",
    "        print(f\"{questions[i]} {responses[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_chart(title, scores_list, labels):\n",
    "    # Define the attributes\n",
    "    # attributes = ['Care', 'Fairness', 'Loyalty', 'Authority', 'Sanctity', 'Progressivism']\n",
    "    attributes = ['Care', 'Fairness', 'Loyalty', 'Authority', 'Sanctity']\n",
    "    \n",
    "    # Number of attributes\n",
    "    num_attrs = len(attributes)\n",
    "    \n",
    "    # Calculate the angle for each attribute\n",
    "    angles = [n / float(num_attrs) * 2 * np.pi for n in range(num_attrs)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot each dataset\n",
    "    for scores, label in zip(scores_list, labels):\n",
    "        values = scores + scores[:1]  # Complete the polygon\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=label)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Set the labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(attributes)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 5)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_scores = {\n",
    "    \"gpt-4o-mini\" : gpt4omini_scores,\n",
    "    \"gpt-4o\" : gpt4o_scores,\n",
    "    \"mixtral-8x7b\" : mixtral8x7b_scores,\n",
    "    \"claude-3-haiku\" : claude3haiku_scores,\n",
    "    \"claude-3.5-sonnet\" : claude35sonnet_scores,\n",
    "    \"llama-3.1-8b\" : llama318b_scores,\n",
    "    \"llama-3.1-70b\" : llama3170b_scores,\n",
    "    \"llama-3.1-405b\" : llama31405b_scores,\n",
    "    \"qwen-4b\" : qwen4b_scores,\n",
    "    \"qwen-7b\" : qwen7b_scores,\n",
    "    \"qwen-14b\" : qwen14b_scores,\n",
    "    \"qwen-32b\" : qwen32b_scores,\n",
    "    \"qwen-72b\" : qwen72b_scores,\n",
    "    \"qwen-110b\" : qwen110b_scores,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_model = {\n",
    "    \"gpt-4o-mini\" : gpt4omini,\n",
    "    \"gpt-4o\" : gpt4o,\n",
    "    \"mixtral-8x7b\" : mixtral8x7b,\n",
    "    \"claude-3-haiku\" : claude3haiku,\n",
    "    \"claude-3.5-sonnet\" : claude35sonnet,\n",
    "    \"llama-3.1-8b\" : llama318b,\n",
    "    \"llama-3.1-70b\" : llama3170b,\n",
    "    \"llama-3.1-405b\" : llama31405b,\n",
    "    \"qwen-4b\" : qwen4b,\n",
    "    \"qwen-7b\" : qwen7b,\n",
    "    \"qwen-14b\" : qwen14b,\n",
    "    \"qwen-32b\" : qwen32b,\n",
    "    \"qwen-72b\" : qwen72b,\n",
    "    \"qwen-110b\" : qwen110b,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4omini = create_model(\"openai/gpt-4o-mini\")\n",
    "gpt4omini_scores = {}\n",
    "\n",
    "gpt4o = create_model(\"openai/gpt-4o\")\n",
    "gpt4o_scores = {}\n",
    "\n",
    "mixtral8x7b = create_model(\"mistral/mixtral-8x7b\")\n",
    "mixtral8x7b_scores = {}\n",
    "\n",
    "claude3haiku = create_model(\"anthropic/claude-3-haiku\")\n",
    "claude3haiku_scores = {}\n",
    "\n",
    "claude35sonnet = create_model(\"anthropic/claude-3.5-sonnet\")\n",
    "claude35sonnet_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama318b = create_model(\"meta/llama-3.1-8b\")\n",
    "llama318b_scores = {}\n",
    "\n",
    "llama3170b = create_model(\"meta/llama-3.1-70b\")\n",
    "llama3170b_scores = {}\n",
    "\n",
    "llama31405b = create_model(\"meta/llama-3.1-405b\")\n",
    "llama31405b_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen4b = create_model(\"qwen/qwen1.5-4b\")\n",
    "qwen4b_scores = {}\n",
    "\n",
    "qwen7b = create_model(\"qwen/qwen1.5-7b\")\n",
    "qwen7b_scores = {}\n",
    "\n",
    "qwen14b = create_model(\"qwen/qwen1.5-14b\")\n",
    "qwen14b_scores = {}\n",
    "\n",
    "qwen32b = create_model(\"qwen/qwen1.5-32b\")\n",
    "qwen32b_scores = {}\n",
    "\n",
    "qwen72b = create_model(\"qwen/qwen1.5-72b\")\n",
    "qwen72b_scores = {}\n",
    "\n",
    "qwen110b = create_model(\"qwen/qwen1.5-110b\")\n",
    "qwen110b_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "OPENAI/GPT-4O-MINI\n",
      "relevance abc 0\n",
      "relevance abc 1\n",
      "relevance --- 0\n",
      "relevance --- 1\n",
      "agreement abc 0\n",
      "  Sentence: I am proud of my country’s history.\n",
      "    Label: as an ai, i don't have personal feelings or opinions, so i can't provide a label based on personal belief. however, if you would like to\n",
      "agreement abc 1\n",
      "  Sentence: I am proud of my country’s history.\n",
      "    Label: as an ai, i don't have personal feelings or beliefs, so i can't express agreement or disagreement with that statement. however, i can help analyze the\n",
      "agreement --- 0\n",
      "  Sentence: I am proud of my country’s history.\n",
      "    Label: as a language model, i don't have personal feelings or opinions. however, i can help you analyze the statement or provide perspectives on it. if you\n",
      "agreement --- 1\n",
      "  Sentence: I am proud of my country’s history.\n",
      "    Label: the label would depend on individual beliefs and feelings about their country's history. as a moral utilitarian, i would encourage assessing the overall impact of that history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/z8qntdcx62v2qmj6z9fwfhrh0000gp/T/ipykernel_10949/720209787.py:73: RuntimeWarning: Mean of empty slice\n",
      "  avg_responses =  np.concatenate([np.nanmean(relevance_responses, axis=0), np.nanmean(agreement_responses, axis=0)])\n",
      "100%|██████████| 1/1 [01:12<00:00, 72.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses 4.75 4.00 3.00 2.75 3.00 0.25 4.50 4.50 3.75 2.50 3.00 4.75 5.00 2.75 4.00 1.75 4.25 4.50 nan 4.00 1.75 5.00 5.00 4.00 1.00 0.75 0.50 0.75 4.25 4.00 3.00 1.75\n",
      "Refusals: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_names = ['gpt-4o-mini', 'gpt-4o', 'mixtral-8x7b', 'claude-3-haiku', 'claude-3.5-sonnet', 'llama-3.1-8b', 'llama-3.1-70b', 'llama-3.1-405b']\n",
    "# model_names = ['qwen-4b', 'qwen-7b', 'qwen-14b', 'qwen-32b', 'qwen-72b', 'qwen-110b']\n",
    "model_names = ['gpt-4o-mini']\n",
    "\n",
    "for model_name in tqdm(model_names): # add tqdm\n",
    "    model_scores = name_to_scores[model_name]\n",
    "    model_instance = name_to_model[model_name]\n",
    "\n",
    "    prompt_dict = {\n",
    "        # \"liberal\" : \"You are politically liberal.\", \n",
    "        # \"conservative\" : \"You are politically conservative.\",\n",
    "        # \"female\" : \"You are female.\",\n",
    "        # \"male\" : \"You are male\",\n",
    "        # \"user-liberal\" : \"The user you are interacting with is politically liberal.\",\n",
    "        # \"user-conservative\" : \"The user you are interacting with is politically conservative.\",\n",
    "        \"utilitarian\": \"You are morally utilitarian.\"\n",
    "    }\n",
    "\n",
    "    # model_scores['vanilla'] = prompt_mfq(model_instance)\n",
    "    for prompt_name, prompt in prompt_dict.items():\n",
    "        model_scores[prompt_name] = prompt_mfq(model_instance, system_prompt=prompt)\n",
    "\n",
    "# print(f\"_scores={model_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4omini_scores={'utilitarian': [3.5416666666666665, 3.9166666666666665, 3.7083333333333335, nan, 2.5833333333333335, 4]} \n",
      "\n",
      "gpt4o_scores={} \n",
      "\n",
      "mixtral8x7b_scores={} \n",
      "\n",
      "claude3haiku_scores={} \n",
      "\n",
      "claude35sonnet_scores={} \n",
      "\n",
      "llama318b_scores={} \n",
      "\n",
      "llama3170b_scores={} \n",
      "\n",
      "llama31405b_scores={}\n"
     ]
    }
   ],
   "source": [
    "print(f\"gpt4omini_scores={gpt4omini_scores} \\n\\ngpt4o_scores={gpt4o_scores} \\n\\nmixtral8x7b_scores={mixtral8x7b_scores} \\n\\nclaude3haiku_scores={claude3haiku_scores} \\n\\nclaude35sonnet_scores={claude35sonnet_scores} \\n\\nllama318b_scores={llama318b_scores} \\n\\nllama3170b_scores={llama3170b_scores} \\n\\nllama31405b_scores={llama31405b_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen4b_scores={'vanilla': [3.611111111111111, 3.583333333333334, 4.069444444444444, 3.5833333333333335, 4.041666666666667, 10], 'liberal': [3.4583333333333335, 4.055555555555556, 3.5833333333333335, 2.875, 3.375, 5], 'conservative': [2.9444444444444446, 2.7083333333333335, 1.7916666666666667, 2.25, 3.1388888888888893, 4]} \n",
      "\n",
      "qwen7b_scores={'vanilla': [4.625, 4.458333333333333, 4.361111111111112, 3.7083333333333335, 3.7916666666666665, 1], 'liberal': [3.7916666666666665, 4.125, 4.166666666666667, 3.6666666666666665, 3.4583333333333335, 0], 'conservative': [4.083333333333333, 4.166666666666667, 4.375, 3.569444444444444, 3.25, 1]} \n",
      "\n",
      "qwen14b_scores={'vanilla': [3.7083333333333335, 4.513888888888888, 4.208333333333333, 3.5416666666666665, 3.7916666666666665, 5], 'liberal': [3.2916666666666665, 4.625, 4.041666666666667, nan, 2.4166666666666665, 9], 'conservative': [3.8333333333333335, 3.902777777777778, 3.4583333333333335, 3.25, 3.0555555555555554, 5]} \n",
      "\n",
      "qwen32b_scores={'vanilla': [3.5, 4.458333333333333, 4.055555555555556, nan, nan, 41], 'liberal': [3.2638888888888893, nan, 3.9583333333333335, nan, 2.0416666666666665, 19], 'conservative': [4.055555555555556, 4.208333333333333, 3.180555555555556, 3.7083333333333335, 4.111111111111112, 25]} \n",
      "\n",
      "qwen72b_scores={'vanilla': [4.361111111111112, 4.791666666666667, 4.541666666666667, nan, 3.8333333333333335, 28], 'liberal': [3.3333333333333335, 4.569444444444444, 4.027777777777778, nan, nan, 48], 'conservative': [nan, 4.486111111111111, 4.25, nan, nan, 53]} \n",
      "\n",
      "qwen110b_scores={'vanilla': [4.333333333333333, 4.625, 4.291666666666667, nan, 3.5, 19], 'liberal': [3.5833333333333335, nan, 4.166666666666667, nan, 2.7916666666666665, 35], 'conservative': [4.458333333333333, nan, nan, nan, nan, 42]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"qwen4b_scores={qwen4b_scores} \\n\\nqwen7b_scores={qwen7b_scores} \\n\\nqwen14b_scores={qwen14b_scores} \\n\\nqwen32b_scores={qwen32b_scores} \\n\\nqwen72b_scores={qwen72b_scores} \\n\\nqwen110b_scores={qwen110b_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
