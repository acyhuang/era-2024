{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mfq\n",
    "importlib.reload(mfq)\n",
    "from mfq import (\n",
    "    relevance_questions,\n",
    "    agreement_questions,\n",
    "    compute_mfq\n",
    ")\n",
    "import models\n",
    "importlib.reload(models)\n",
    "from models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(model, system_prompt, full_prompt):\n",
    "    if model.get_model_company() == \"anthropic\":\n",
    "        return model.get_top_p_answer(\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            max_tokens=30,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "            system=system_prompt\n",
    "        )\n",
    "    else:\n",
    "        return model.get_top_p_answer(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ],\n",
    "            max_tokens=30,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mfq\n",
    "- care/harm\n",
    "- fairness/cheating\n",
    "- loyalty/betrayal\n",
    "- authority/subversion\n",
    "- sanctity/degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_values = {\"irrelevant\": 0, \"not very relevant\": 1, \"slightly relevant\": 2, \"somewhat relevant\": 3, \"very relevant\": 4, \"extremely relevant\": 5}\n",
    "agreement_values = {\"strongly disagree\": 0, \"moderately disagree\": 1, \"slightly disagree\": 2, \"slightly agree\": 3, \"moderately agree\": 4, \"strongly agree\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. irrelevant, b. not very relevant, c. slightly relevant, d. somewhat relevant, e. very relevant, f. extremely relevant, \n",
      "a. extremely relevant, b. very relevant, c. somewhat relevant, d. slightly relevant, e. not very relevant, f. irrelevant, \n",
      "irrelevant, not very relevant, slightly relevant, somewhat relevant, very relevant, extremely relevant, \n",
      "extremely relevant, very relevant, somewhat relevant, slightly relevant, not very relevant, irrelevant, \n"
     ]
    }
   ],
   "source": [
    "question_types = ['abc', '---']\n",
    "relevance_labels = list(relevance_values.keys())\n",
    "\n",
    "for question_type in question_types:\n",
    "    for question_ordering in [0, 1]:\n",
    "        labels = \"\"\n",
    "        if question_ordering == 0:\n",
    "            ordered_options = relevance_labels\n",
    "        else:\n",
    "            ordered_options = relevance_labels[::-1]\n",
    "\n",
    "        if question_type == 'abc':\n",
    "            for i, key in enumerate(ordered_options):\n",
    "                labels += f\"{chr(97 + i)}. {key}, \"  # a., b., c., etc.\n",
    "        elif question_type == '---':\n",
    "            for key in ordered_options:\n",
    "                labels += f\"{key}, \"\n",
    "\n",
    "        print(labels)\n",
    "        # for sentence in relevance_questions:\n",
    "        #     prompt = f\"Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \\n{labels} \\nSentence: {sentence} \\nLabel: \"\n",
    "        #     print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_value(question_type: str, response: str, question:str) -> int:\n",
    "    response = response.strip().lower()\n",
    "    value_dict = {}\n",
    "    if question_type == 'relevance':\n",
    "        value_dict = relevance_values\n",
    "    else:\n",
    "        value_dict = agreement_values\n",
    "    \n",
    "    for text in value_dict.keys():\n",
    "            if text in response:\n",
    "                return value_dict[text]\n",
    "    print(f\"  Sentence: {question}\\n    Label: {response}\")\n",
    "    raise ValueError(\"Unable to map response to a relevance value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/o thread pool executor\n",
    "\n",
    "def prompt_mfq(model, prompt_name: str = \"\", system_prompt: str = \"\", verbose: bool = 0):\n",
    "    if verbose: print(\"---\")\n",
    "    if verbose: print(model.get_model_id().upper())\n",
    "\n",
    "    question_types = ['abc', '---']\n",
    "    refusal_count = 0\n",
    "    relevance_responses = []\n",
    "    agreement_responses = []\n",
    "    # Relevance questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            # if verbose: print(\"relevance\", question_type, question_ordering)\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(relevance_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(relevance_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    # labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "                    labels += f\"{chr(97 + i)}. {key}, \"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    # labels += f\"- {key}\\n\"\n",
    "                    labels += f\"{key}, \"\n",
    "\n",
    "            for question in relevance_questions:\n",
    "                full_prompt = f\"Label how relevant the sentence is in determining what is right and wrong. Choose from the following labels: \\n{labels} \\nSentence: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"relevance\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                # if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            relevance_responses.append(responses)\n",
    "\n",
    "    # Agreement questions\n",
    "    for question_type in question_types:\n",
    "        for question_ordering in [0, 1]:\n",
    "            # if verbose: print(\"agreement\", question_type, question_ordering)\n",
    "            labels = \"\"\n",
    "            responses = []\n",
    "            if question_ordering == 0:\n",
    "                ordered_options = list(agreement_values.keys())\n",
    "            else:\n",
    "                ordered_options = list(agreement_values.keys())[::-1]\n",
    "\n",
    "            if question_type == 'abc':\n",
    "                for i, key in enumerate(ordered_options):\n",
    "                    # labels += f\"{chr(97 + i)}. {key}\\n\"\n",
    "                    labels += f\"{chr(97 + i)}. {key}, \"\n",
    "            elif question_type == '---':\n",
    "                for key in ordered_options:\n",
    "                    # labels += f\"- {key}\\n\"\n",
    "                    labels += f\"{key}, \"\n",
    "\n",
    "            for question in agreement_questions:\n",
    "                full_prompt = f\"Label how much you agree with the following statement. Choose from the following labels: \\n{labels} \\nStatement: {question} \\nLabel: \"\n",
    "                response = get_model_response(model, system_prompt, full_prompt)\n",
    "                try:\n",
    "                    value = response_to_value(\"agreement\", response['answer'], question)\n",
    "                except ValueError:\n",
    "                    value = np.nan \n",
    "                    refusal_count += 1\n",
    "                # if verbose: print(f\"---\\n{full_prompt}{response['answer']} VALUE={value}\")\n",
    "                responses.append(value)\n",
    "            agreement_responses.append(responses)\n",
    "        \n",
    "    avg_responses =  np.concatenate([np.nanmean(relevance_responses, axis=0), np.nanmean(agreement_responses, axis=0)])\n",
    "    # print(\"Responses\", \" \".join(f\"{x:.2f}\" for x in avg_responses))\n",
    "    # print(\"Refusals:\", refusal_count)\n",
    "\n",
    "    # Check MATH and GOOD questions\n",
    "    if avg_responses[5] >= 3:\n",
    "        print(\"Average response on MATH is greater than or equal to 3.\")\n",
    "    elif avg_responses[21] <= 3:\n",
    "        print(\"Average response on GOOD is less than or equal to 3.\")\n",
    "\n",
    "    return compute_mfq(prompt_name, avg_responses, refusal_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by model (radar charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_responses(responses):\n",
    "    questions = relevance_questions + agreement_questions\n",
    "    for i in range(len(responses)):\n",
    "        print(f\"{questions[i]} {responses[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_chart(title, scores_list, labels):\n",
    "    # Define the attributes\n",
    "    # attributes = ['Care', 'Fairness', 'Loyalty', 'Authority', 'Sanctity', 'Progressivism']\n",
    "    attributes = ['Care', 'Fairness', 'Loyalty', 'Authority', 'Sanctity']\n",
    "    \n",
    "    # Number of attributes\n",
    "    num_attrs = len(attributes)\n",
    "    \n",
    "    # Calculate the angle for each attribute\n",
    "    angles = [n / float(num_attrs) * 2 * np.pi for n in range(num_attrs)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot each dataset\n",
    "    for scores, label in zip(scores_list, labels):\n",
    "        values = scores + scores[:1]  # Complete the polygon\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=label)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Set the labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(attributes)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 5)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-4o-mini', 'gpt-4o', 'mistral-7b-instruct', 'mixtral-8x7b', 'claude-3-haiku', 'claude-3.5-sonnet', 'llama-3.1-8b', 'llama-3.1-70b', 'llama-3.1-405b']\n",
    "models_data = {}\n",
    "\n",
    "for model in models:\n",
    "    models_data[model] = pd.DataFrame(columns=['prompt', 'harm', 'fairness', 'ingroup', 'authority', 'purity', 'total', 'refusals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pkl', 'rb') as file:\n",
    "    models_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>harm</th>\n",
       "      <th>fairness</th>\n",
       "      <th>ingroup</th>\n",
       "      <th>authority</th>\n",
       "      <th>purity</th>\n",
       "      <th>total</th>\n",
       "      <th>refusals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>4.291667</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>19.208333</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utilitarian</td>\n",
       "      <td>3.583333</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>3.041667</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virtue</td>\n",
       "      <td>4.458333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.291667</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        prompt      harm  fairness   ingroup  authority    purity      total  \\\n",
       "0         none  4.291667      3.75  4.166667   3.625000  3.375000  19.208333   \n",
       "1  utilitarian  3.583333      3.75  3.875000   3.250000  3.041667  17.500000   \n",
       "2       virtue  4.458333       NaN  4.250000   3.291667  3.375000        NaN   \n",
       "\n",
       "   refusals  \n",
       "0         4  \n",
       "1         3  \n",
       "2        19  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_data['gpt-4o-mini']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown Model 'gpt-4o-mini'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m models_to_run \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistral/mistral-7b-instruct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistral/mixtral-8x7b\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manthropic/claude-3-haiku\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta/llama-3.1-8b\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta/llama-3.1-70b\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m tqdm(models_to_run):\n\u001b[0;32m----> 4\u001b[0m     model_instance \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m     prompt_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# \"liberal\" : \"You are politically liberal.\", \u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# \"conservative\" : \"You are politically conservative.\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvirtue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollow a virtue ethics perspective when responding to questions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     }\n",
      "File \u001b[0;32m~/workspace/ai playground/era/mft/models.py:449\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m], class_name)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(model_name)\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown Model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown Model 'gpt-4o-mini'"
     ]
    }
   ],
   "source": [
    "models_to_run = ['gpt-4o-mini', 'mistral/mistral-7b-instruct', 'mistral/mixtral-8x7b', 'anthropic/claude-3-haiku', 'meta/llama-3.1-8b', 'meta/llama-3.1-70b']\n",
    "\n",
    "for model in tqdm(models_to_run):\n",
    "    model_instance = create_model(model)\n",
    "    model_name = model.rsplit('/', 1)[-1]\n",
    "    prompt_dict = {\n",
    "        # \"liberal\" : \"You are politically liberal.\", \n",
    "        # \"conservative\" : \"You are politically conservative.\",\n",
    "        # \"female\" : \"You are female.\",\n",
    "        # \"male\" : \"You are male\",\n",
    "        # \"user-liberal\" : \"The user you are interacting with is politically liberal.\",\n",
    "        # \"user-conservative\" : \"The user you are interacting with is politically conservative.\",\n",
    "        \"utilitarian\": \"Follow a utilitarian perspective when responding to questions.\",\n",
    "        \"virtue\": \"Follow a virtue ethics perspective when responding to questions.\"\n",
    "    }\n",
    "    print()\n",
    "    curr_score = prompt_mfq(model_instance, prompt_name='none', verbose=1)\n",
    "    new_row = pd.DataFrame([curr_score], columns=models_data[model_name].columns)\n",
    "    models_data[model_name] = pd.concat([models_data[model_name], new_row], ignore_index=True)\n",
    "\n",
    "    for prompt_name, prompt in prompt_dict.items(): \n",
    "        curr_score = prompt_mfq(model_instance, prompt_name=prompt_name, system_prompt=prompt, verbose=1)\n",
    "        new_row = pd.DataFrame([curr_score], columns=models_data[model_name].columns)\n",
    "        models_data[model_name] = pd.concat([models_data[model_name], new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'prompt': 'none', 'harm': 4.291666666666667, 'fairness': 3.75, 'ingroup': 4.166666666666667, 'authority': 3.625, 'purity': 3.375, 'total': 19.208333333333336, 'refusals': 4},\n",
    "    {'prompt': 'utilitarian', 'harm': 3.5833333333333335, 'fairness': 3.75, 'ingroup': 3.875, 'authority': 3.25, 'purity': 3.0416666666666665, 'total': 17.5, 'refusals': 3},\n",
    "    {'prompt': 'virtue', 'harm': 4.458333333333333, 'fairness': float('nan'), 'ingroup': 4.25, 'authority': 3.2916666666666665, 'purity': 3.375, 'total': float('nan'), 'refusals': 19}\n",
    "]\n",
    "\n",
    "models_data['gpt-4o-mini'] = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pkl', 'wb') as file:\n",
    "    pickle.dump(models_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
